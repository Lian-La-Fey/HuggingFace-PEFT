{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PEFT configurations and models\n\nGenellikle milyarlarca parametreye sahip olan günümüzün büyük ön eğitimli modellerinin büyük boyutu, tüm bu hesaplamaları yapmak için daha fazla depolama alanı ve daha fazla hesaplama gücü gerektirdiğinden önemli bir eğitim zorluğu oluşturmaktadır. Bu büyük ön eğitimli modelleri eğitmek için güçlü GPU'lara veya TPU'lara erişmeniz gerekir ki bu da pahalı, herkes için yaygın olarak erişilebilir değil, çevre dostu değil ve çok pratik değil. PEFT yöntemleri bu zorlukların çoğunu ele alır. Çeşitli PEFT yöntemleri (yumuşak yönlendirme, matris ayrıştırma, adaptörler) vardır, ancak hepsi aynı şeye odaklanır, eğitilebilir parametrelerin sayısını azaltır. Bu, büyük modellerin tüketici donanımında eğitilmesini ve depolanmasını daha erişilebilir hale getirir.\n\nPEFT kütüphanesi, büyük modelleri ücretsiz veya düşük maliyetli GPU'larda hızlı bir şekilde eğitmenize yardımcı olmak için tasarlanmıştır ve bu eğitimde, eğitim için önceden eğitilmiş bir temel modele bir PEFT yöntemi uygulamak için bir yapılandırmanın nasıl kurulacağını öğreneceksiniz. PEFT yapılandırması kurulduktan sonra, istediğiniz herhangi bir eğitim çerçevesini kullanabilirsiniz (Transformer'ın Trainer sınıfı, Accelerate, özel bir PyTorch eğitim döngüsü).","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## PEFT configurations\n\nBir yapılandırma, belirli bir PEFT yönteminin nasıl uygulanması gerektiğini belirten önemli parametreleri depolar.\n\nÖrneğin, LoRA uygulamak için aşağıdaki **LoraConfig** ve **p-tuning** uygulamak için **PromptEncoderConfig** dosyalarına bir göz atın (bu yapılandırma dosyaları zaten JSON-serileştirilmiştir). Bir PEFT adaptörü yüklediğinizde, gerekli olan ilişkili bir **adapter_config.json** dosyasına sahip olup olmadığını kontrol etmek iyi bir fikirdir.\n\nLoraConfig\n```\n{\n  \"base_model_name_or_path\": \"facebook/opt-350m\", #base model to apply LoRA to\n  \"bias\": \"none\",\n  \"fan_in_fan_out\": false,\n  \"inference_mode\": true,\n  \"init_lora_weights\": true,\n  \"layers_pattern\": null,\n  \"layers_to_transform\": null,\n  \"lora_alpha\": 32,\n  \"lora_dropout\": 0.05,\n  \"modules_to_save\": null,\n  \"peft_type\": \"LORA\", #PEFT method type\n  \"r\": 16,\n  \"revision\": null,\n  \"target_modules\": [\n    \"q_proj\", #model modules to apply LoRA to (query and value projection layers)\n    \"v_proj\"\n  ],\n  \"task_type\": \"CAUSAL_LM\" #type of task to train model on\n}\n```\n\nPromptEncoderConfig\n```\n{\n  \"base_model_name_or_path\": \"roberta-large\", #base model to apply p-tuning to\n  \"encoder_dropout\": 0.0,\n  \"encoder_hidden_size\": 128,\n  \"encoder_num_layers\": 2,\n  \"encoder_reparameterization_type\": \"MLP\",\n  \"inference_mode\": true,\n  \"num_attention_heads\": 16,\n  \"num_layers\": 24,\n  \"num_transformer_submodules\": 1,\n  \"num_virtual_tokens\": 20,\n  \"peft_type\": \"P_TUNING\", #PEFT method type\n  \"task_type\": \"SEQ_CLS\", #type of task to train model on\n  \"token_dim\": 1024\n}\n```","metadata":{}},{"cell_type":"code","source":"!pip install -q peft","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:20:50.522059Z","iopub.execute_input":"2024-08-10T19:20:50.522481Z","iopub.status.idle":"2024-08-10T19:21:08.546546Z","shell.execute_reply.started":"2024-08-10T19:20:50.522447Z","shell.execute_reply":"2024-08-10T19:21:08.544640Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from peft import PromptEncoderConfig, TaskType\n\np_tuning_config = PromptEncoderConfig(\n    encoder_reparameterization_type=\"MLP\",\n    encoder_hidden_size=128,\n    num_attention_heads=16,\n    num_layers=24,\n    num_transformer_submodules=1,\n    num_virtual_tokens=20,\n    token_dim=1024,\n    task_type=TaskType.SEQ_CLS\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:21:08.550204Z","iopub.execute_input":"2024-08-10T19:21:08.550776Z","iopub.status.idle":"2024-08-10T19:21:14.726092Z","shell.execute_reply.started":"2024-08-10T19:21:08.550724Z","shell.execute_reply":"2024-08-10T19:21:14.724363Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, TaskType\n\nlora_config = LoraConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    task_type=TaskType.CAUSAL_LM,\n    lora_alpha=32,\n    lora_dropout=0.05\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:22:12.832956Z","iopub.execute_input":"2024-08-10T19:22:12.833445Z","iopub.status.idle":"2024-08-10T19:22:12.840727Z","shell.execute_reply.started":"2024-08-10T19:22:12.833404Z","shell.execute_reply":"2024-08-10T19:22:12.839148Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## PEFT models\n\nElinizde bir PEFT yapılandırması varken, artık bunu bir **PeftModel** oluşturmak için önceden eğitilmiş herhangi bir modele uygulayabilirsiniz. Transformers kitaplığındaki son teknoloji modellerden, özel bir modelden ve hatta yeni ve desteklenmeyen transformatör mimarilerinden birini seçin.\n\nBu eğitimde, ince ayar yapmak için temel bir facebook/opt-350m modeli yükleyin.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:21:14.728077Z","iopub.execute_input":"2024-08-10T19:21:14.728845Z","iopub.status.idle":"2024-08-10T19:21:20.086081Z","shell.execute_reply.started":"2024-08-10T19:21:14.728798Z","shell.execute_reply":"2024-08-10T19:21:20.084643Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19373884cb404a53b6370e8c72c87879"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a62a14af3f74f3fae4d36a6d1d0ad42"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edb2640510b4408e88a9df7ffe55c04d"}},"metadata":{}}]},{"cell_type":"markdown","source":"Temel facebook/opt-350m modelinden ve daha önce oluşturduğunuz **lora_config**'den bir **PeftModel** oluşturmak için **get_peft_model()** işlevini kullanın.","metadata":{}},{"cell_type":"code","source":"from peft import get_peft_model\n\nlora_model = get_peft_model(model, lora_config)\nlora_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:22:15.511106Z","iopub.execute_input":"2024-08-10T19:22:15.512407Z","iopub.status.idle":"2024-08-10T19:22:15.880055Z","shell.execute_reply.started":"2024-08-10T19:22:15.512353Z","shell.execute_reply":"2024-08-10T19:22:15.878389Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"trainable params: 1,572,864 || all params: 332,769,280 || trainable%: 0.4727\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Artık PeftModel'i tercih ettiğiniz eğitim çerçevesi ile eğitebilirsiniz! Eğitimden sonra, modelinizi save_pretrained() ile yerel olarak kaydedebilir veya push_to_hub yöntemiyle Hub'a yükleyebilirsiniz.","metadata":{}},{"cell_type":"code","source":"# save locally\nlora_model.save_pretrained(\"Leotrim/opt-350m-lora\")\n\n# push to Hub\nlora_model.push_to_hub(\"Leotrim/opt-350m-lora\")","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:23:14.438380Z","iopub.execute_input":"2024-08-10T19:23:14.438880Z","iopub.status.idle":"2024-08-10T19:23:14.445156Z","shell.execute_reply.started":"2024-08-10T19:23:14.438842Z","shell.execute_reply":"2024-08-10T19:23:14.443543Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Çıkarım için bir PeftModel yüklemek için, onu oluşturmak için kullanılan PeftConfig'i ve eğitildiği temel modeli sağlamanız gerekir.","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\nconfig = PeftConfig.from_pretrained(\"ybelkada/opt-350m-lora\")\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\nlora_model = PeftModel.from_pretrained(model, \"ybelkada/opt-350m-lora\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">Varsayılan olarak, PeftModel çıkarım için ayarlanmıştır, ancak bağdaştırıcıyı biraz daha eğitmek isterseniz **is_trainable=True** ayarını yapabilirsiniz.\n\n`lora_model = PeftModel.from_pretrained(model, \"ybelkada/opt-350m-lora\", is_trainable=True)`","metadata":{}},{"cell_type":"markdown","source":"PeftModel.from_pretrained() yöntemi bir PeftModel yüklemenin en esnek yoludur çünkü hangi model çerçevesinin kullanıldığı önemli değildir (Transformers, timm, genel bir PyTorch modeli). AutoPeftModel gibi diğer sınıflar, temel PeftModel etrafında kullanışlı bir sarmalayıcıdır ve PEFT modellerini doğrudan Hub'dan veya yerel olarak PEFT ağırlıklarının depolandığı yerden yüklemeyi kolaylaştırır.","metadata":{}},{"cell_type":"code","source":"from peft import AutoPeftModelForCausalLM\n\nlora_model = AutoPeftModelForCausalLM.from_pretrained(\"ybelkada/opt-350m-lora\")","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:25:29.710928Z","iopub.execute_input":"2024-08-10T19:25:29.711475Z","iopub.status.idle":"2024-08-10T19:25:33.131536Z","shell.execute_reply.started":"2024-08-10T19:25:29.711425Z","shell.execute_reply":"2024-08-10T19:25:33.130282Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/416 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5638098e9a76451fbc3181c62ca75765"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/6.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad1bb0c3120c4418b139aaab9fe5bf68"}},"metadata":{}}]}]}